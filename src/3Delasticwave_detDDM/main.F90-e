!! Main Code: By Ajit Desai, Augest/2015, PETSc Version : March/2016
!! purpose: is to define inputs, load FEM/DDM data and call DDM solvers
!

!!!!! Elastic wave propagation :
!!! ADAPTED BY SUDHI FROM MAIN CODE OF AJIT DESAI


!  MESH-Data:
!      p e t : points: elements: triangles: for local or each subdomain
!      np ne nt     : number of points, elements & triangles for local or each subdomain
!      pg eg tg     : points: elements: triangles: for global or whole domain
!      npg neg ntg  : number of points, elements & triangles for global or whole domain
!      nb nci nri   : number of nodes on boundary, corner & remaining for local or each subdomain
!      nbg nbgc nbgr: number of nodes on boundary, corner & remaining for global or whole domain
!      dbounds : physical domain limits : x=[0, 1], y=[0,1] : for unit square
!
!  MPI-Data:
!      pid   : Processor ID
!      nproc : number of processors
!      ierr  : MPI errors
!
!  FEM-Data:
!      Amat  : FEM assembly matrix
!      Aadv  : advection matrix
!      Adif  : diffusion matrix
!      fvec  : FEM assembly vector
!
!  DDM-Data:
!      Aii   : interior-interior sub-block of Amat
!      Aig   : interior-boundary sub-block of Amat
!      Air   : interior-remainig sub-block of Amat
!      Aic   : interior-corner sub-block of Amat
!      Fi Fg : interior & boundary sub-block of Fvec
!
! output:
!      Ui Ur Uc : local interior, remaining & corner nodes solution vector
!      Ub Ub_g  : local & global boundary nodes solution vector
!!
!!---------------------------------------------------------------------------------------
PROGRAM main

    use common
    use myCommon
    use assembly
    use PETScSolvers

    implicit none

include 'mpif.h'

!!------------------------------------------------------
!! Two-Level-NNC-PCGM with PETSc Sparse Iterative Solver
!!------------------------------------------------------

!!MPI-Data:
    integer :: ierr, pid
    integer, parameter :: nVec = 3 !!For 3D

!!Exit-Criteria
    integer, parameter :: maxiter = 100
    double precision, parameter :: tol = 1.0d-6
    integer, parameter :: outputFlag=1


!!MESH-Data-Inputs:
    integer    :: npg, neg, ntg, nhg, nbg, nbgc, nbgr, nParts
    integer    :: np, ne, nh, nt, nb, nci, nri
    character(len=255) :: extension, label
    integer, allocatable, dimension(:,:)          :: hg
    double precision, allocatable, dimension(:,:) :: pg
    integer, allocatable, dimension(:,:)          :: eg
    integer, allocatable, dimension(:,:)          :: tg

!!FEM & DDM-Data-Outputs:
    double precision, allocatable, dimension(:)   :: Ui, Ub, Ub_g, U_gi, U_g

!! Newmark Beta/Time discretization Parameters

    double precision :: T, deltaT, beta_NB, gamma_NB
    integer     :: tcount

!!PCKF-Data-Inputs/Outputs                                                       !!** PCKF ***
    integer :: nmeasg  !nmeas
    double precision, allocatable, dimension(:,:) :: Af, Afb
    double precision, allocatable, dimension(:)   :: var_g !uveci, uvecb
    !integer, allocatable, dimension(:) :: measnodes  !!,measnodesg
    !double precision, allocatable, dimension(:)   :: d

!!Time Calculation Data:
    integer :: c1,c2,cr
    double precision :: time1
    double precision :: time2
    !integer :: seed_size
    !integer, dimension (:), allocatable :: seed

!!-----------------------------------------------------------------------------------------------
!! MPI initiation
    CALL MPI_INIT(ierr)
    CALL MPI_COMM_RANK(MPI_COMM_WORLD,pid,ierr)

!!-----------------------------------------------------------------------------------------------
!!!!!!!!!!> 1: Pre-Processing: Memory-Allocation / Assembly / Distribution of Amat
!!-----------------------------------------------------------------------------------------------
!!*** Pre-Processing Time T1 ***!!!
    call system_clock(count_rate=cr)
    call cpu_time(time1)
    call system_clock(c1)


!!-----------------------------------------------------------------------------------------------
!! Read mesh data : local & global
    if (pid .eq. 0) then

        call readGlobMeshDim(nParts)

        extension = ''
        nmeasg = 0
        !!call readmeshdim(npg,neg,ntg,nbg,extension)
        call readmeshdim3D(npg,neg,ntg,nhg,nbg,extension)
        print*, 'nNodes    =',npg
        call readgcrdim(nbgc,nbgr)
        allocate(pg(3,npg),hg(4,nhg))
        call readmeshdata3D(pg,hg,npg,nhg,extension)
        call readNB_Para(T,deltaT,beta_NB,gamma_NB,extension)

        print*, '--------------------------------------'
        print*, 'nParts             =',nParts
        print*, 'nNodes             =',npg
        print*, 'nBoundary Nodes    =',nbg
        print*, 'nCorner Nodes      =',nbgc
        print*, 'nRemaing Nodes     =',nbgr


        print*,'***************************************************'
        print*,'******** Time discretization Parameters ***********'
        print*,'***************************************************'

        print*, 'Total Time, T      =',T
        print*, 'deltaT             =',deltaT

    end if


    CALL MPI_BCAST(nbg,1,MPI_INTEGER,0,MPI_COMM_WORLD,ierr)
    CALL MPI_BCAST(npg,1,MPI_INTEGER,0,MPI_COMM_WORLD,ierr)
    CALL MPI_BCAST(nbgr,1,MPI_INTEGER,0,MPI_COMM_WORLD,ierr)
    CALL MPI_BCAST(nbgc,1,MPI_INTEGER,0,MPI_COMM_WORLD,ierr)
    CALL MPI_BCAST(nmeasg,1,MPI_INTEGER,0,MPI_COMM_WORLD,ierr)               !!** PCKF ***

    CALL MPI_BCAST(T,1,MPI_DOUBLE_PRECISION,0,MPI_COMM_WORLD,ierr)
    CALL MPI_BCAST(deltaT,1,MPI_DOUBLE_PRECISION,0,MPI_COMM_WORLD,ierr)
    CALL MPI_BCAST(beta_NB,1,MPI_DOUBLE_PRECISION,0,MPI_COMM_WORLD,ierr)
    CALL MPI_BCAST(gamma_NB,1,MPI_DOUBLE_PRECISION,0,MPI_COMM_WORLD,ierr)

    call int2str(extension,pid+1,4)
    extension = trim(extension)
    call readmeshdim3D(np,ne,nt,nh,nb,extension)          !!**
    !!call readmeshdim(np,ne,nt,nb,extension)
    call readcrdim(nci,nri,extension)
    !!call readnmeas(nmeas,extension)                                               !!** PCKF ***
    if (pid .eq. 0) then
        print*, '---------------------------------------------------------------'
        if (nci > nri) then
            ! Commented out by sudhi since determining WB and VB grid
            ! can't be done just by checking nci and nri in one subdomain
            print*,'Number of corner nodes',nci
            print*,'Number of remaining nodes',nri
            ! print*, "Using wire-basket grid"
        else
            print*,'Number of corner nodes',nci
            print*,'Number of remaining nodes',nri
        end if
    end if


    tcount = NINT(T/deltaT)    ! Total time steps
!!-----------------------------------------------------------------------------------------------
!! Dynamic array allocation
    allocate(Ub(nb*nVec)) !!p(2,np),e(3,ne),t(3,nt),
    allocate(Ui((np-nb)*nVec),Ub_g(nbg*nVec),U_g(npg*nVec),U_gi(npg*nVec))
    allocate(Af(np*nVec,1),Afb(nbg*nVec,1),var_g(npg*nVec))
    !!allocate(uveci(np-nb),uvecb(nbg),d(nmeas),measnodes(nmeas))                   !!** PCKF ***

    !! allocate(Ur(nri*npceout),Uc(nci*npceout),UiUr((np-nci)*npceout),&
    !! FETIuveci(np-nci),FETIuvecb(nbgc),FETIAfb(nbgc,npceout+nmeasg),Uc_g(nbgc*npceout))

!!!-----------------------------------------------------------------------------------------------
!!!!!!!!!> 2: Calling Solver Only for Mallocs Calculations
!!!-----------------------------------------------------------------------------------------------
!! If you want to precalculate mallocs "mallocsCals = 1'      : First time run : Necessory
!! If have calculated mallocs then just use "mallocsCals = 0" : Repeated runs
!! mallocsCals = 1

!    if (mallocsCals .eq. 1) then
!
!    call PETSc_stonncpcgm(pid,p,e,t,np,ne,nt,nb,nbg,ndim,npcein,npceout,nri,nci,nbgc,nomga,ncijk,&
!                    ijk,cijk,mIndex,sIndex,casep,dbounds,const_diff,omegas,multipliers,sigma,amp,&
!                    Ui,Ub,Ub_g,mallocsCals,maxiter,tol)
!    end if

!!!-----------------------------------------------------------------------------------------------
!!*** Pre-Processing Time T2 ***!!!
    if (pid .eq. 0) then
        call cpu_time(time2)
        call system_clock(c2)
        if (pid .eq. 0) print*, '---------------------------------------------------------------'
        print*, 'Time taken for pre-processing', dble(c2-c1)/dble(cr),'seconds'
        print*, 'CPU-Time for pre-processing', time2-time1,'seconds'
        if (pid .eq. 0) print*, '---------------------------------------------------------------'
    end if

!!!-----------------------------------------------------------------------------------------------
!!*** Processing Time T1 ***!!! This includes assembly
    call cpu_time(time1)
    call system_clock(c1)

!!-----------------------------------------------------------------------------------------------
!!!!!!!!> 3: Main Process: Calling Solver
!!-----------------------------------------------------------------------------------------------


    ! call PETSc_stonncpcgm(pid,np,nb,nbg,npcein,npceout,nri,nci,nbgc,ncijk,ijk,cijk,Ui,Ub,Ub_g, &
    !                     nVec,mallocsCals,maxiter,tol)


    call PETSc_detnncpcgm(pid,np,nb,npg,nbg,nri,nci,nbgc,Ui,Ub,Ub_g,nVec,maxiter,tol,T, tcount, deltaT, &
        beta_NB, gamma_NB,outputFlag)



!!-----------------------------------------------------------------------------------------------
!!!-----------------------------------------------------------------------------------------------
!!*** Processing Time T2 ***!!!
    if (pid .eq. 0) then
        call cpu_time(time2)
        call system_clock(c2)
        if (pid .eq. 0) print*, '---------------------------------------------------------'
        print*, 'Time taken by solver', dble(c2-c1)/dble(cr),'seconds'
        print*, 'CPU-Time by solver',time2-time1,'seconds'
        if (pid .eq. 0) print*, '---------------------------------------------------------'
    end if



!-----------------------------------------------------------------------------------------------
!! PCGM-Post-Processing
!!-----------------------------------------------------------------------------------------------
!         call create_Af_pckfddm3D(nmeasg,npceout,np,nb,Ui,Ub,nVec,Af)

!         if (pid .eq. 0) then
!           call create_Afb_pckfddm3D(nmeasg,npceout,nbg,Ub_g,nVec,Afb)
!         end if

!         !xi(:) = 0.0d0
!         !xi(1) = -1.0d0

! !!!***** Mean *****!!!
!        call construct_coln3D(pid,np,nb,npg,Af(1:(np-nb)*nVec,1),U_gi)
!        !!call construct_coln3D(pid,np,nb,npg,Ui,U_gi)

!        CALL MPI_REDUCE(U_gi,U_g,npg*nVec,MPI_DOUBLE_PRECISION,MPI_SUM,0,MPI_COMM_WORLD,ierr)
!        if (pid .eq. 0) then
!           call construct_coln_boundary3D(nbg,npg,Afb(:,1),U_g)
!           !!label='../../data/vtkOutputs/out_mean_prior_pce.vtk'
!           !!call writevtk(label,pg,tg,U_g,npg,ntg)
!           !!print*, maxval(U_g)

!           !print*, 'Max-mean =', maxval(U_g)
!        end if

!         if (pid .eq. 0) then
!             open(unit=2,file='../../data/vtkOutputs/mean_solutionVector.dat',status='replace')
!             write(unit=2,fmt='(ES17.8E3)') U_g
!             close(2)
!         end if


!         U_g(:) = 0.0d0
!         if (pid .eq. 0) then
!             call construct_coln_boundary3D(nbg,npg,Afb(:,1),U_g)
!             if (pid .eq. 0) then
!                 open(unit=2,file='../../data/vtkOutputs/mean_GlobSolutionVector.dat',status='replace')
!                 write(unit=2,fmt='(ES17.8E3)') U_g
!                 close(2)
!             end if
!         end if


! !!!***** SD *****!!!
!         var_g(:) = 0.0d0
!         do i = 2,npceout
!         call construct_coln3D(pid,np,nb,npg,Ui((i-1)*(np-nb)*nVec+1:i*(np-nb)*nVec),U_gi)
!         call MPI_REDUCE(U_gi,U_g,npg*nVec,MPI_DOUBLE_PRECISION,MPI_SUM,0,MPI_COMM_WORLD,ierr)
!             if (pid .eq. 0) then
!                 call construct_coln_boundary3D(nbg,npg,Ub_g((i-1)*nbg*nVec+1:i*nbg*nVec),U_g)
!                 var_g = var_g + U_g**2

!                 !!!***** Higher Order PCE Coefficients *****!!!
!                 call int2str(extension,i,2)
!                 label= '../../data/vtkOutputs/pce_' // trim(extension) // '.dat'
!                 open(unit=22,file=label,status='replace')
!                 write(unit=22,fmt='(ES17.8E3)') U_g
!                 close(22)

!             end if
!         end do

!         if (pid .eq. 0) then
!             !!!***** SD *****!!!
!             var_g = sqrt(var_g)
!             !!label='../../data/vtkOutputs/out_sd_prior_pce.vtk'
!             !!call writevtk(label,pg,tg,var_g,npg,ntg)

!             !print*, "Max-SD   =", maxval(var_g)

!             open(unit=2,file='../../data/vtkOutputs/sd_solutionVector.dat',status='replace')
!             write(unit=2,fmt='(ES17.8E3)') var_g
!             close(2)

!         end if

!!-----------------------------------------------------------------------------------------------
!! Dynamic array de-allocation
   if (pid .eq. 0) then
      deallocate(pg,hg)
   end if

    deallocate(Ub,Ui,Ub_g,U_g,U_gi)
    deallocate(Af,Afb,var_g) !,uveci,uvecb,,measnodes,d)                          !!** PCKF ***

   CALL MPI_FINALIZE(ierr)


END PROGRAM main
!!%%%%%%%%%%%%%%%%%%%%%%%%%%%%** END **%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
!!%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
